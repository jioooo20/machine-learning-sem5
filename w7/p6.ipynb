{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cdea36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact NN done in 292.800 s\n",
      "Annoy done in 27.246 s\n",
      "HNSW done in 36.224 s\n",
      "FAISS IVF done in 51.168 s\n",
      "\n",
      "Top-5 neighbors for first song:\n",
      "Exact NN: [     0 394553 764272 837727 749223]\n",
      "Annoy:    [0, 764272, 833164, 676733, 523698]\n",
      "HNSW:     [     0 394553 764272 837727 749223]\n",
      "FAISS:    [     0 394553 764272 837727 749223]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "import hnswlib\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -------------------------------\n",
    "# Load dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv('songs_with_attributes_and_lyrics.csv')  # ganti path sesuai lokasi file\n",
    "features = ['danceability', 'energy', 'loudness', 'speechiness', \n",
    "            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
    "X = df[features].values\n",
    "\n",
    "# Standarisasi fitur\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "k = 10  # jumlah nearest neighbors\n",
    "\n",
    "# -------------------------------\n",
    "# Exact Nearest Neighbor (brute-force)\n",
    "# -------------------------------\n",
    "start = time.time()\n",
    "nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "nn.fit(X_scaled)\n",
    "dist_exact, idx_exact = nn.kneighbors(X_scaled)\n",
    "time_exact = time.time() - start\n",
    "print(f\"Exact NN done in {time_exact:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# Annoy\n",
    "# -------------------------------\n",
    "start = time.time()\n",
    "f = X_scaled.shape[1]\n",
    "index_annoy = AnnoyIndex(f, 'euclidean')\n",
    "for i, v in enumerate(X_scaled):\n",
    "    index_annoy.add_item(i, v)\n",
    "index_annoy.build(10)\n",
    "idx_annoy = [index_annoy.get_nns_by_vector(v, k) for v in X_scaled]\n",
    "time_annoy = time.time() - start\n",
    "print(f\"Annoy done in {time_annoy:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# HNSW\n",
    "# -------------------------------\n",
    "start = time.time()\n",
    "p_hnsw = hnswlib.Index(space='l2', dim=X_scaled.shape[1])\n",
    "p_hnsw.init_index(max_elements=X_scaled.shape[0], ef_construction=200, M=16)\n",
    "p_hnsw.add_items(X_scaled)\n",
    "p_hnsw.set_ef(200)\n",
    "idx_hnsw, dist_hnsw = p_hnsw.knn_query(X_scaled, k=k)\n",
    "time_hnsw = time.time() - start\n",
    "print(f\"HNSW done in {time_hnsw:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# FAISS IVF\n",
    "# -------------------------------\n",
    "start = time.time()\n",
    "quantizer = faiss.IndexFlatL2(X_scaled.shape[1])\n",
    "nlist = 100\n",
    "index_faiss = faiss.IndexIVFFlat(quantizer, X_scaled.shape[1], nlist, faiss.METRIC_L2)\n",
    "index_faiss.train(X_scaled)\n",
    "index_faiss.add(X_scaled)\n",
    "index_faiss.nprobe = 10\n",
    "dist_faiss, idx_faiss = index_faiss.search(X_scaled, k)\n",
    "time_faiss = time.time() - start\n",
    "print(f\"FAISS IVF done in {time_faiss:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# Contoh tampilkan top-5 neighbors dari item pertama\n",
    "# -------------------------------\n",
    "print(\"\\nTop-5 neighbors for first song:\")\n",
    "print(f\"Exact NN: {idx_exact[0][:5]}\")\n",
    "print(f\"Annoy:    {idx_annoy[0][:5]}\")\n",
    "print(f\"HNSW:     {idx_hnsw[0][:5]}\")\n",
    "print(f\"FAISS:    {idx_faiss[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9e592",
   "metadata": {},
   "source": [
    "Buat dan tuliskan analisa anda terhadap code diatas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0876f919",
   "metadata": {},
   "source": [
    "Kode diatas membandingkan 4 metode ANN pada dataset lagu spotify. tujuan dari kode ini yaitu membandingkan kecepatan dan akurasi dari metode Exat NN, annoy, hnsw, dan faiss ivf.\n",
    "\n",
    "pada bagian pertama dilakukan preprocessing dengan fungsi StandardScaler pada fitur X\n",
    "\n",
    "Pada metode pertama menggunakan ENN dengan matrix distance Euclidean, method ini membandingkan setiap titik ke titik lainnya, sehingga untuk dataset besar akan terlalu lama prosesnya, namun akurasi mencapai 100%.\n",
    "\n",
    "pada metode kedua menggunakan Annoy, method ini membuat 10 tree untuk membagi ruang vector, hasil proses jauh lebih cepat menjadi 27,246 detik, namun bisa ada tetangga yang terlewat, hasil tetangga juga sedikit berbeda dengan ENN karena approximate.\n",
    "\n",
    "pada metode ketiga menggunakan HNSW, metode ini menggunakan graph-based indexing, setiap vector dihubungkan ke vector lain yang dekat sehingga seperti membentuk jaring jarring, hasil dari method ini sama persis dengan ENN dengan akurasi 100%, untuk kecepatan juga jauh lebih baik menjadi 36,224 detik dari ENN.\n",
    "\n",
    "pada method keempat menggunakan FAISS IVF, method ini membagi data menjadi beberapa cluster(nlist), saat query hanya cluster terdekat(nprobe) cluster yang diperiksa, hasil tetangga juga sama persis dengan ENN dan kecepatannya juga jauh lebih baik menjadi 51,168 detik dari ENN dengan akurasi 100%\n",
    "\n",
    "kesimpulannya, hasil dari ENN paling akurat, namun Waktu proses nya sangat lama untuk dataset besar sehingga tidak cocok untuk kondisi realtime. annoy menghasilkan kecepatan yang paling cepat, cocok untuk rekomendasi lagu seperti spotify namun akurasi yang kurang sempurna. HNSW merupakan metode ayng seimbang, hasil sama persis dengan ENN pada konfigurasi yang tepat, sehingga cocok untuk sistem besar dengan kebutuhan akurasi tinggi. Terakhir FAISS IVF lebih cocok digunakan untuk dataset yang sangat besar sehingga cocok digunakan untuk skala industri, contohnya META.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985f7ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
